{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83e8453-0cab-4790-a41a-2bd52bda8f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 10:11:19.225512: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-03 10:11:19.269698: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-03 10:11:19.930238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007044792175292969,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6d2aa3b0f94e04968f0eccc84f814f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from biopandas.pdb import PandasPdb\n",
    "from biopandas.mmcif import PandasMmcif\n",
    "import torch\n",
    "import dgl\n",
    "import tokenizers\n",
    "import transformers\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "# 分层特征处理架构（代码示例）\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_aaindex1(file_path):\n",
    "    aaindex1_df = pd.read_csv(file_path, index_col='Description')\n",
    "    aaindex_dict = {aa: aaindex1_df[aa].values for aa in aaindex1_df.columns}\n",
    "    return aaindex_dict\n",
    "\n",
    "def extract_features(sequence, aaindex_dict):\n",
    "    features = []\n",
    "    for aa in sequence:\n",
    "        if aa in aaindex_dict:\n",
    "            features.append(aaindex_dict[aa])\n",
    "        else:\n",
    "            features.append(np.full((len(next(iter(aaindex_dict.values()))),), np.nan))\n",
    "    return np.array(features)\n",
    "\n",
    "def generate_graph(coords, threshold=8.0):\n",
    "    all_diffs = np.expand_dims(coords, axis=1) - np.expand_dims(coords, axis=0)\n",
    "    distance = np.sqrt(np.sum(np.power(all_diffs, 2), axis=-1))\n",
    "    adj = distance < threshold\n",
    "    u, v = np.nonzero(adj)\n",
    "    u, v = torch.from_numpy(u), torch.from_numpy(v)\n",
    "    graph = dgl.graph((u, v), num_nodes=coords.shape[0])\n",
    "    return graph\n",
    "\n",
    "#model = \"/root/autodl-tmp/ToxGIN-main/ESM\"\n",
    "model = \"facebook/esm2_t36_3B_UR50D\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
    "config.hidden_dropout = 0.\n",
    "config.hidden_dropout_prob = 0.\n",
    "config.attention_dropout = 0.\n",
    "config.attention_probs_dropout_prob = 0.\n",
    "encoder = AutoModel.from_pretrained(model, config=config).to(device).eval()\n",
    "print(\"model loaded\")\n",
    "\n",
    "def seq_encode(seq):\n",
    "    spaced_seq = \" \".join(list(seq))\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        spaced_seq, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True,\n",
    "        max_length=60,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = encoder(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    last_hidden_states = outputs[0]\n",
    "    encoded_seq = last_hidden_states[inputs['attention_mask'].bool()][1:-1]\n",
    "    return encoded_seq\n",
    "\n",
    "def aggre(s):\n",
    "    if type(s.values[0]) == str:\n",
    "        return s.values[0]\n",
    "    return np.mean(s)\n",
    "\n",
    "aa_map = {'VAL': 'V', 'PRO': 'P', 'ASN': 'N', 'GLU': 'E', 'ASP': 'D', 'ALA': 'A', 'THR': 'T', 'SER': 'S',\n",
    "          'LEU': 'L', 'LYS': 'K', 'GLY': 'G', 'GLN': 'Q', 'ILE': 'I', 'PHE': 'F', 'CYS': 'C', 'TRP': 'W',\n",
    "          'ARG': 'R', 'TYR': 'Y', 'HIS': 'H', 'MET': 'M'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6f9192-0199-415d-9541-0c40ca27fcd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#压缩模块,可选/调参数\n",
    "def compress(encode,features):\n",
    "    variances = np.var(encode, axis=0)\n",
    "    keep_dim = int(encode.shape[1] * 0.01)\n",
    "    top_idx = np.argsort(variances)[-keep_dim:][::-1]\n",
    "    preserved_feat = encode[:, top_idx]    \n",
    "    residual_feat = np.delete(encode, top_idx, axis=1)\n",
    "    scaler = RobustScaler(unit_variance=True)\n",
    "    scaled_residual = scaler.fit_transform(residual_feat)    \n",
    "    safe_dim = min(scaled_residual.shape[0]-1, scaled_residual.shape[1])\n",
    "    pca = PCA(n_components=min(0.5, safe_dim), svd_solver='full')\n",
    "    compressed_residual = pca.fit_transform(scaled_residual)\n",
    "    esm_compressed = np.hstack([preserved_feat, compressed_residual])\n",
    "    esm_var = np.var(esm_compressed, axis=0)\n",
    "    top100_idx = np.argsort(esm_var)[-50:][::-1]\n",
    "    esm_compressed = esm_compressed[:, top100_idx]\n",
    "\n",
    "    \n",
    "    chosen_lis = [22,23,108,111,132,142,143,144,146,147,151,163,398]\n",
    "    # var_selector = VarianceThreshold(4)\n",
    "    # aaindex_filtered = var_selector.fit_transform(features)\n",
    "    # aaindex_var = np.var(aaindex_filtered, axis=0)\n",
    "    # top100_idx = np.argsort(aaindex_var)[-100:][::-1]\n",
    "    # aaindex_compressed = aaindex_filtered[:, top100_idx]\n",
    "    if esm_compressed.shape[1]+aaindex_compressed.shape[1]<150:\n",
    "        print(esm_compressed.shape,aaindex_compressed.shape)\n",
    "    return esm_compressed,aaindex_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787fc0f7-e368-49c0-8cc7-5a64bb6c4c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import dgl\n",
    "\n",
    "\n",
    "# 确保输出目录存在\n",
    "df = pd.read_csv('train_sequence.csv')\n",
    "root_dir = 'train_data'\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "graphs = []\n",
    "pdb = None\n",
    "\n",
    "aaindex_dict = load_aaindex1('aaindex1.csv')\n",
    "scaler = MinMaxScaler()\n",
    "graph_dir = Path(root_dir) / 'dgl_graphs'\n",
    "graph_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "valid_indices = []  # 记录有效处理的索引，后续可能需要更新数据框\n",
    "train_data = {\n",
    "    'target': df['label'].tolist(),\n",
    "    'atoms': [],\n",
    "    'coordinates': [],\n",
    "    'features': [],\n",
    "    'res_coords':[]\n",
    "}\n",
    "for index, row in df.iterrows():\n",
    "    sequence = row.sequence\n",
    "    pdb_file = f'train_structures/{sequence}.pdb'\n",
    "    \n",
    "    # 检查PDB文件是否存在\n",
    "    if not os.path.exists(pdb_file):\n",
    "        print(f\"PDB文件 {pdb_file} 不存在，跳过样本 {sequence}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # 读取PDB文件\n",
    "        atom_df = PandasPdb().read_pdb(pdb_file).df['ATOM']\n",
    "\n",
    "        residue_df = atom_df.groupby('residue_number', as_index=False).agg(aggre).sort_values('residue_number')     \n",
    "        # 生成残基名称序列\n",
    "        residue_df['letter'] = residue_df.residue_name.map(aa_map)\n",
    "        pdb_seq = ''.join(residue_df.letter.values)\n",
    "        atom_df = atom_df.merge(residue_df[['residue_number', 'letter']],\n",
    "                                on='residue_number', how='left')        \n",
    "        # 验证生成的序列是否与预期一致\n",
    "        if pdb_seq != sequence:\n",
    "            print(f\"序列不匹配: 文件{pdb_file}生成序列 {pdb_seq} 不等于预期序列 {sequence}\")\n",
    "            continue\n",
    "        \n",
    "        # 生成分子图\n",
    "        res_coords = residue_df[['x_coord', 'y_coord', 'z_coord']].values\n",
    "        graph = generate_graph(res_coords)\n",
    "        \n",
    "        # 验证图节点数等于序列长度\n",
    "        if graph.number_of_nodes() != len(pdb_seq):\n",
    "            print(f\"图节点数{graph.num_nodes()} 与序列长度{len(pdb_seq)}不一致\")\n",
    "            continue\n",
    "        graphs.append(graph) \n",
    "            \n",
    "        # 获取原子信息和坐标\n",
    "        atom_types = atom_df['element_symbol'].tolist()  # 原子类型（C, N, O 等）\n",
    "        coords = atom_df[['x_coord', 'y_coord', 'z_coord']].values  # 原子坐标\n",
    "        # 存储原子和坐标信息\n",
    "        train_data['atoms'].append(atom_types)\n",
    "        train_data['coordinates'].append(coords)\n",
    "        # 生成特征向量\n",
    "        encoded_pdb_seq = seq_encode(pdb_seq).cpu().numpy()\n",
    "        features = extract_features(pdb_seq, aaindex_dict)\n",
    "        # encoded_pdb_seq,features = compress(encoded_pdb_seq,features)\n",
    "        # normalized_encoded = scaler.fit_transform(encoded_pdb_seq.astype(np.float32))\n",
    "        # normalized_features = scaler.fit_transform(features.astype(np.float32))\n",
    "        combined_features = np.concatenate([encoded_pdb_seq, features], axis=-1)\n",
    "\n",
    "        # 保存特征文件\n",
    "        npz_path = os.path.join(root_dir, f\"{sequence}.npz\")\n",
    "        np.savez_compressed(npz_path, wildtype_seq=combined_features)\n",
    "        \n",
    "        # 保存图文件\n",
    "        graph_path = os.path.join(graph_dir, f\"{sequence}.bin\")\n",
    "        dgl.save_graphs(graph_path, [graph])\n",
    "        \n",
    "        valid_indices.append(index)\n",
    "        feature = [];coord_lis=[];index=0\n",
    "        for letter in atom_df['letter']:\n",
    "            if letter!=pdb_seq[index]:\n",
    "                index += 1\n",
    "            feature.append(combined_features[index])\n",
    "            coord_lis.append(res_coords[index])\n",
    "        if len(feature)!=len(atom_df):\n",
    "            print(len(feature),len(atom_df))\n",
    "            break\n",
    "        train_data['features'].append(feature)\n",
    "        train_data['res_coords'].append(coord_lis)\n",
    "        del atom_df\n",
    "    except Exception as e:\n",
    "        print(f\"处理样本 {sequence} 时发生错误: {str(e)}\")\n",
    "        continue\n",
    "dgl.save_graphs(root_dir+'/dgl_graph.bin', graphs)\n",
    "# 可选: 更新数据框，仅保留有效样本\n",
    "df_valid = df.loc[valid_indices].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d53b7c-e7b1-4bfe-8549-8036dad218b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import dgl\n",
    "\n",
    "\n",
    "# 确保输出目录存在\n",
    "df = pd.read_csv('test_sequence.csv')\n",
    "root_dir = 'test_data'\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "graphs = []\n",
    "pdb = None\n",
    "\n",
    "aaindex_dict = load_aaindex1('aaindex1.csv')\n",
    "scaler = MinMaxScaler()\n",
    "graph_dir = Path(root_dir) / 'dgl_graphs'\n",
    "graph_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "valid_indices = []  # 记录有效处理的索引，后续可能需要更新数据框\n",
    "test_data = {\n",
    "    'target': df['label'].tolist(),\n",
    "    'atoms': [],\n",
    "    'coordinates': [],\n",
    "    'features': [],\n",
    "    'res_coords':[]\n",
    "}\n",
    "for index, row in df.iterrows():\n",
    "    sequence = row.sequence\n",
    "    pdb_file = f'test_structures/{sequence}.pdb'\n",
    "    \n",
    "    # 检查PDB文件是否存在\n",
    "    if not os.path.exists(pdb_file):\n",
    "        print(f\"PDB文件 {pdb_file} 不存在，跳过样本 {sequence}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # 读取PDB文件\n",
    "        atom_df = PandasPdb().read_pdb(pdb_file).df['ATOM']\n",
    "        residue_df = atom_df.groupby('residue_number', as_index=False).agg(aggre).sort_values('residue_number')\n",
    "        \n",
    "        \n",
    "        # 生成残基名称序列\n",
    "        residue_df['letter'] = residue_df.residue_name.map(aa_map)\n",
    "        pdb_seq = ''.join(residue_df.letter.values)\n",
    "        atom_df = atom_df.merge(residue_df[['residue_number', 'letter']],\n",
    "                                on='residue_number', how='left')         \n",
    "        # 验证生成的序列是否与预期一致\n",
    "        if pdb_seq != sequence:\n",
    "            print(f\"序列不匹配: 文件{pdb_file}生成序列 {pdb_seq} 不等于预期序列 {sequence}\")\n",
    "            continue\n",
    "        \n",
    "        # 生成分子图\n",
    "        res_coords = residue_df[['x_coord', 'y_coord', 'z_coord']].values\n",
    "        graph = generate_graph(res_coords)\n",
    "        \n",
    "        # 验证图节点数等于序列长度\n",
    "        if graph.number_of_nodes() != len(pdb_seq):\n",
    "            print(f\"图节点数{graph.num_nodes()} 与序列长度{len(pdb_seq)}不一致\")\n",
    "            continue\n",
    "        graphs.append(graph)     \n",
    "        # 获取原子信息和坐标\n",
    "        atom_types = atom_df['element_symbol'].tolist()  # 原子类型（C, N, O 等）\n",
    "        coords = atom_df[['x_coord', 'y_coord', 'z_coord']].values  # 原子坐标\n",
    "        # 存储原子和坐标信息\n",
    "        test_data['atoms'].append(atom_types)\n",
    "        test_data['coordinates'].append(coords)\n",
    "        # 生成特征向量\n",
    "        encoded_pdb_seq = seq_encode(pdb_seq).cpu().numpy()\n",
    "        features = extract_features(pdb_seq, aaindex_dict)\n",
    "        # encoded_pdb_seq,features = compress(encoded_pdb_seq,features)\n",
    "        # normalized_encoded = scaler.fit_transform(encoded_pdb_seq.astype(np.float32))\n",
    "        # normalized_features = scaler.fit_transform(features.astype(np.float32))\n",
    "        combined_features = np.concatenate([encoded_pdb_seq, features], axis=-1)\n",
    "        \n",
    "        # 保存特征文件\n",
    "        npz_path = os.path.join(root_dir, f\"{sequence}.npz\")\n",
    "        np.savez_compressed(npz_path, wildtype_seq=combined_features)\n",
    "        \n",
    "        # 保存图文件\n",
    "        graph_path = os.path.join(graph_dir, f\"{sequence}.bin\")\n",
    "        dgl.save_graphs(graph_path, [graph])\n",
    "        valid_indices.append(index)\n",
    "        feature = [];coord_lis=[];index=0\n",
    "        for letter in atom_df['letter']:\n",
    "            if letter!=pdb_seq[index]:\n",
    "                index += 1\n",
    "            feature.append(combined_features[index])\n",
    "            coord_lis.append(res_coords[index])\n",
    "        if len(feature)!=len(atom_df):\n",
    "            print(len(feature),len(atom_df))\n",
    "            break\n",
    "        test_data['features'].append(feature)\n",
    "        test_data['res_coords'].append(coord_lis)\n",
    "    except Exception as e:\n",
    "        print(f\"处理样本 {sequence} 时发生错误: {str(e)}\")\n",
    "        continue\n",
    "dgl.save_graphs(root_dir+'/dgl_graph.bin', graphs)\n",
    "# 可选: 更新数据框，仅保留有效样本\n",
    "df_valid = df.loc[valid_indices].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe7411-beb2-4d30-ab3c-342ecb1730f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 2560) (15, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(6, 2560) (6, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(7, 2560) (7, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(6, 2560) (6, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(5, 2560) (5, 553)\n",
      "(5, 2560) (5, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(14, 2560) (14, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(15, 2560) (15, 553)\n",
      "(5, 2560) (5, 553)\n",
      "(6, 2560) (6, 553)\n",
      "(7, 2560) (7, 553)\n",
      "(8, 2560) (8, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(9, 2560) (9, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(10, 2560) (10, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(11, 2560) (11, 553)\n",
      "(12, 2560) (12, 553)\n",
      "(13, 2560) (13, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(37, 2560) (37, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(35, 2560) (35, 553)\n",
      "(35, 2560) (35, 553)\n",
      "(35, 2560) (35, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(33, 2560) (33, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(31, 2560) (31, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(37, 2560) (37, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(31, 2560) (31, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(58, 2560) (69, 553)\n",
      "处理样本 GFFGNTWKKIKGKADKIMLKKAVKIMVKKEGISKEEAQAKVDAMSKKQIRLYLLKYYGKKALQKASEKL 时发生错误: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 58 and the array at index 1 has size 69\n",
      "(18, 2560) (18, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(31, 2560) (31, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(37, 2560) (37, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(32, 2560) (32, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(31, 2560) (31, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(33, 2560) (33, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(32, 2560) (32, 553)\n",
      "(32, 2560) (32, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(36, 2560) (36, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(34, 2560) (34, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(56, 2560) (56, 553)\n",
      "(34, 2560) (34, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(57, 2560) (57, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(33, 2560) (33, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(33, 2560) (33, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(31, 2560) (31, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(18, 2560) (18, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(45, 2560) (45, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(38, 2560) (38, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(20, 2560) (20, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(28, 2560) (28, 553)\n",
      "(45, 2560) (45, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(25, 2560) (25, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(16, 2560) (16, 553)\n",
      "(44, 2560) (44, 553)\n",
      "(45, 2560) (45, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(22, 2560) (22, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(27, 2560) (27, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(30, 2560) (30, 553)\n",
      "(31, 2560) (31, 553)\n",
      "(31, 2560) (31, 553)\n",
      "(41, 2560) (41, 553)\n",
      "(45, 2560) (45, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(48, 2560) (48, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(31, 2560) (31, 553)\n",
      "(29, 2560) (29, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(26, 2560) (26, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(24, 2560) (24, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(23, 2560) (23, 553)\n",
      "(21, 2560) (21, 553)\n",
      "(19, 2560) (19, 553)\n",
      "(17, 2560) (17, 553)\n",
      "(27, 2560) (27, 553)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import dgl\n",
    "\n",
    "\n",
    "# 确保输出目录存在\n",
    "df = pd.read_csv('tamper_data.csv')\n",
    "root_dir = 'tamper_data'\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "graphs = []\n",
    "pdb = None\n",
    "\n",
    "aaindex_dict = load_aaindex1('aaindex1.csv')\n",
    "scaler = MinMaxScaler()\n",
    "graph_dir = Path(root_dir) / 'dgl_graphs'\n",
    "graph_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "valid_indices = []  # 记录有效处理的索引，后续可能需要更新数据框\n",
    "test_data = {\n",
    "    'target': df['label'].tolist(),\n",
    "    'atoms': [],\n",
    "    'coordinates': [],\n",
    "    'features': [],\n",
    "    'res_coords':[]\n",
    "}\n",
    "for index, row in df.iterrows():\n",
    "    sequence = row.sequence\n",
    "    pdb_file = f'tamper_data/{sequence}.pdb'\n",
    "    \n",
    "    # 检查PDB文件是否存在\n",
    "    if not os.path.exists(pdb_file):\n",
    "        print(f\"PDB文件 {pdb_file} 不存在，跳过样本 {sequence}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # 读取PDB文件\n",
    "        atom_df = PandasPdb().read_pdb(pdb_file).df['ATOM']\n",
    "        residue_df = atom_df.groupby('residue_number', as_index=False).agg(aggre).sort_values('residue_number')\n",
    "        \n",
    "        \n",
    "        # 生成残基名称序列\n",
    "        residue_df['letter'] = residue_df.residue_name.map(aa_map)\n",
    "        pdb_seq = ''.join(residue_df.letter.values)\n",
    "        atom_df = atom_df.merge(residue_df[['residue_number', 'letter']],\n",
    "                                on='residue_number', how='left')         \n",
    "        # 验证生成的序列是否与预期一致\n",
    "        if pdb_seq != sequence:\n",
    "            print(f\"序列不匹配: 文件{pdb_file}生成序列 {pdb_seq} 不等于预期序列 {sequence}\")\n",
    "            continue\n",
    "        \n",
    "        # 生成分子图\n",
    "        res_coords = residue_df[['x_coord', 'y_coord', 'z_coord']].values\n",
    "        graph = generate_graph(res_coords)\n",
    "        \n",
    "        # 验证图节点数等于序列长度\n",
    "        if graph.number_of_nodes() != len(pdb_seq):\n",
    "            print(f\"图节点数{graph.num_nodes()} 与序列长度{len(pdb_seq)}不一致\")\n",
    "            continue\n",
    "        graphs.append(graph)     \n",
    "        # 获取原子信息和坐标\n",
    "        atom_types = atom_df['element_symbol'].tolist()  # 原子类型（C, N, O 等）\n",
    "        coords = atom_df[['x_coord', 'y_coord', 'z_coord']].values  # 原子坐标\n",
    "        # 存储原子和坐标信息\n",
    "        test_data['atoms'].append(atom_types)\n",
    "        test_data['coordinates'].append(coords)\n",
    "        # 生成特征向量\n",
    "        encoded_pdb_seq = seq_encode(pdb_seq).cpu().numpy()\n",
    "        features = extract_features(pdb_seq, aaindex_dict)\n",
    "        # encoded_pdb_seq,features = compress(encoded_pdb_seq,features)\n",
    "        # normalized_encoded = scaler.fit_transform(encoded_pdb_seq.astype(np.float32))\n",
    "        # normalized_features = scaler.fit_transform(features.astype(np.float32))\n",
    "        print(encoded_pdb_seq.shape,features.shape)\n",
    "        combined_features = np.concatenate([encoded_pdb_seq, features], axis=-1)\n",
    "        \n",
    "        # 保存特征文件\n",
    "        npz_path = os.path.join(root_dir, f\"{sequence}.npz\")\n",
    "        np.savez_compressed(npz_path, wildtype_seq=combined_features)\n",
    "        \n",
    "        # 保存图文件\n",
    "        graph_path = os.path.join(graph_dir, f\"{sequence}.bin\")\n",
    "        dgl.save_graphs(graph_path, [graph])\n",
    "        valid_indices.append(index)\n",
    "        feature = [];coord_lis=[];index=0\n",
    "        for letter in atom_df['letter']:\n",
    "            if letter!=pdb_seq[index]:\n",
    "                index += 1\n",
    "            feature.append(combined_features[index])\n",
    "            coord_lis.append(res_coords[index])\n",
    "        if len(feature)!=len(atom_df):\n",
    "            print(len(feature),len(atom_df))\n",
    "            break\n",
    "        test_data['features'].append(feature)\n",
    "        test_data['res_coords'].append(coord_lis)\n",
    "    except Exception as e:\n",
    "        print(f\"处理样本 {sequence} 时发生错误: {str(e)}\")\n",
    "        continue\n",
    "dgl.save_graphs(root_dir+'/dgl_graph.bin', graphs)\n",
    "# 可选: 更新数据框，仅保留有效样本\n",
    "df_valid = df.loc[valid_indices].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2883663-0fef-43ea-8b54-19183e97cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import dgl\n",
    "\n",
    "\n",
    "# 确保输出目录存在\n",
    "df = pd.read_csv('test_sequence.csv')\n",
    "root_dir = 'test_data'\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "graphs = []\n",
    "pdb = None\n",
    "\n",
    "aaindex_dict = load_aaindex1('aaindex1.csv')\n",
    "scaler = MinMaxScaler()\n",
    "graph_dir = Path(root_dir) / 'dgl_graphs'\n",
    "graph_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "valid_indices = []  # 记录有效处理的索引，后续可能需要更新数据框\n",
    "test_data = {\n",
    "    'target': df['label'].tolist(),\n",
    "    'atoms': [],\n",
    "    'coordinates': [],\n",
    "    'features': [],\n",
    "    'res_coords':[]\n",
    "}\n",
    "for index, row in df.iterrows():\n",
    "    sequence = row.sequence\n",
    "    pdb_file = f'test_structures/{sequence}.pdb'\n",
    "    \n",
    "    # 检查PDB文件是否存在\n",
    "    if not os.path.exists(pdb_file):\n",
    "        print(f\"PDB文件 {pdb_file} 不存在，跳过样本 {sequence}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # 读取PDB文件\n",
    "        atom_df = PandasPdb().read_pdb(pdb_file).df['ATOM']\n",
    "        residue_df = atom_df.groupby('residue_number', as_index=False).agg(aggre).sort_values('residue_number')\n",
    "        \n",
    "        \n",
    "        # 生成残基名称序列\n",
    "        residue_df['letter'] = residue_df.residue_name.map(aa_map)\n",
    "        pdb_seq = ''.join(residue_df.letter.values)\n",
    "        atom_df = atom_df.merge(residue_df[['residue_number', 'letter']],\n",
    "                                on='residue_number', how='left')         \n",
    "        # 验证生成的序列是否与预期一致\n",
    "        if pdb_seq != sequence:\n",
    "            print(f\"序列不匹配: 文件{pdb_file}生成序列 {pdb_seq} 不等于预期序列 {sequence}\")\n",
    "            continue\n",
    "        \n",
    "        # 生成分子图\n",
    "        res_coords = residue_df[['x_coord', 'y_coord', 'z_coord']].values\n",
    "        graph = generate_graph(res_coords)\n",
    "        \n",
    "        # 验证图节点数等于序列长度\n",
    "        if graph.number_of_nodes() != len(pdb_seq):\n",
    "            print(f\"图节点数{graph.num_nodes()} 与序列长度{len(pdb_seq)}不一致\")\n",
    "            continue\n",
    "        graphs.append(graph)     \n",
    "        # 获取原子信息和坐标\n",
    "        atom_types = atom_df['element_symbol'].tolist()  # 原子类型（C, N, O 等）\n",
    "        coords = atom_df[['x_coord', 'y_coord', 'z_coord']].values  # 原子坐标\n",
    "        # 存储原子和坐标信息\n",
    "        test_data['atoms'].append(atom_types)\n",
    "        test_data['coordinates'].append(coords)\n",
    "        # 生成特征向量\n",
    "        encoded_pdb_seq = seq_encode(pdb_seq).cpu().numpy()\n",
    "        features = extract_features(pdb_seq, aaindex_dict)\n",
    "        # encoded_pdb_seq,features = compress(encoded_pdb_seq,features)\n",
    "        # normalized_encoded = scaler.fit_transform(encoded_pdb_seq.astype(np.float32))\n",
    "        # normalized_features = scaler.fit_transform(features.astype(np.float32))\n",
    "        combined_features = np.concatenate([encoded_pdb_seq, features], axis=-1)\n",
    "        \n",
    "        # 保存特征文件\n",
    "        npz_path = os.path.join(root_dir, f\"{sequence}.npz\")\n",
    "        np.savez_compressed(npz_path, wildtype_seq=combined_features)\n",
    "        \n",
    "        # 保存图文件\n",
    "        graph_path = os.path.join(graph_dir, f\"{sequence}.bin\")\n",
    "        dgl.save_graphs(graph_path, [graph])\n",
    "        valid_indices.append(index)\n",
    "        feature = [];coord_lis=[];index=0\n",
    "        for letter in atom_df['letter']:\n",
    "            if letter!=pdb_seq[index]:\n",
    "                index += 1\n",
    "            feature.append(combined_features[index])\n",
    "            coord_lis.append(res_coords[index])\n",
    "        if len(feature)!=len(atom_df):\n",
    "            print(len(feature),len(atom_df))\n",
    "            break\n",
    "        test_data['features'].append(feature)\n",
    "        test_data['res_coords'].append(coord_lis)\n",
    "    except Exception as e:\n",
    "        print(f\"处理样本 {sequence} 时发生错误: {str(e)}\")\n",
    "        continue\n",
    "dgl.save_graphs(root_dir+'/dgl_graph.bin', graphs)\n",
    "# 可选: 更新数据框，仅保留有效样本\n",
    "df_valid = df.loc[valid_indices].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263f7094-e830-4050-bf1a-1036d6a5ab40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict1, dict2):\n",
    "    result = dict1.copy()   # 复制dict1以避免修改原始数据\n",
    "    for key, value in dict2.items():\n",
    "        if key in result:\n",
    "            # 如果键存在于结果字典中，则合并列表，并去除重复项\n",
    "            result[key] = list(result[key] + value)\n",
    "        else:\n",
    "            # 如果键不存在于结果字典中，则直接添加\n",
    "            result[key] = value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7bbeb5-837d-4c43-b7b6-c87bc4c56e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data_raw.npy',merge_dicts(train_data,test_data))\n",
    "#np.save('train_data_raw.npy', train_data)\n",
    "#np.save('tamper_data.npy', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3dfb4-0b40-4dfa-9270-d6e6e74c3c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_data = np.load('train_data.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543ac96-3ee2-4cb0-b12e-00a9de1ad91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 通过Python强制刷新缓存\n",
    "import os, ctypes,torch\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "libc.sync()\n",
    "if hasattr(libc, 'syscall'):\n",
    "    libc.syscall(ctypes.c_long(162), ctypes.c_long(1))  # sys_syncfs\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
